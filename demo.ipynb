{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "209612cf",
   "metadata": {},
   "source": [
    "# LLM Applications With Redis Enterprise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32b2b86",
   "metadata": {},
   "source": [
    "In this demo we'll show 3 common use cases for Redis Enterprise in LLM applications:\n",
    "1. **Semantic Search** (i.e., Vector Search), and **RAG (Retrieval-Augmented Generation)** to chat with a knowledge base\n",
    "2. **Semantic Cache**\n",
    "3. **Chat Memory**\n",
    "\n",
    "We'll use [LangChain](https://www.langchain.com/) to compose these use cases. You can sign up for a free Redis database [here](https://redis.com/try-free/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f80ac14",
   "metadata": {},
   "source": [
    "The diagram below shows the demo architecture.\n",
    "\n",
    "![](vss-vw-demo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b3e3e0",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc70abe-fa70-4800-b58e-42174e7641d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install redis langchain rich spacy google-cloud-aiplatform unstructured markdown python-dotenv requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5560214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faae6be4",
   "metadata": {},
   "source": [
    "Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4c7a4eb-b703-4880-8553-d8e57b8b15e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print # this will pretty-print python objects\n",
    "import warnings\n",
    "import dotenv\n",
    "\n",
    "# mute warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# load env vars from .env file\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "def download_file(url, filename):\n",
    "    import requests\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open(filename, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610c9d24",
   "metadata": {},
   "source": [
    "## 0. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0b373c",
   "metadata": {},
   "source": [
    "### Load Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bca13ad",
   "metadata": {},
   "source": [
    "Let's talk to the Redis documentation. We'll load a local copy of the Search [Aggregations](https://redis.io/docs/interact/search-and-query/search/aggregations/) and [Query](https://redis.io/docs/interact/search-and-query/query/) pages and use them to answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7235d10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/eli.cohen/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/eli.cohen/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loaded <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> documents\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loaded \u001b[1;36m2\u001b[0m documents\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load documents\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "# download aggregation doc\n",
    "aggs_url = \"https://github.com/RediSearch/RediSearch/raw/master/docs/docs/advanced-concepts/aggregations.md\"\n",
    "aggs_doc_path = \"aggregations.md\"\n",
    "download_file(aggs_url, aggs_doc_path)\n",
    "docs = UnstructuredMarkdownLoader(aggs_doc_path).load()\n",
    "\n",
    "# download query syntax doc\n",
    "query_syntax_url = \"https://github.com/RediSearch/RediSearch/raw/master/docs/docs/advanced-concepts/query_syntax.md\"\n",
    "query_doc_path = \"query_syntax.md\"\n",
    "download_file(query_syntax_url, query_doc_path)\n",
    "docs.extend(UnstructuredMarkdownLoader(query_doc_path).load())\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3c5570",
   "metadata": {},
   "source": [
    "### Split Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e16c4d4",
   "metadata": {},
   "source": [
    "Next, we'll split the doument into chunks and index each chunk as a separate document.\n",
    "\n",
    "This will allow us to retrieve specific, smaller, relevant chunks of the document to add context to our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db2cad70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 841, which is longer than the specified 750\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Generated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">67</span> splits\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Generated \u001b[1;36m67\u001b[0m splits\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split documents into chunks\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "text_splitter = SpacyTextSplitter(chunk_size=750, chunk_overlap=50, strip_whitespace=True)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "print(f\"Generated {len(splits)} splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f242c",
   "metadata": {},
   "source": [
    "### Create Embeddings, Load Into Redis and Create Search Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeb1f0f",
   "metadata": {},
   "source": [
    "Let's create our embeddings transfromer. We will use it to transform our documents, the user's questions, and our prompts into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5830869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "\n",
    "# Define Text Embeddings model\n",
    "embedding = VertexAIEmbeddings(model_name=\"textembedding-gecko\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdb9f7c",
   "metadata": {},
   "source": [
    "We can now use the embeddings object to transform our documents content, then load the documents into Redis.\n",
    "\n",
    "This step will also create a search index called `redis-vs-docs` on the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62489946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 309 ms, sys: 29.9 ms, total: 339 ms\n",
      "Wall time: 1.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create embeddings and load data into Redis\n",
    "from langchain.vectorstores import Redis\n",
    "\n",
    "vectordb = Redis.from_documents(documents=splits, embedding=embedding, index_name=\"redis-vs-docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7792474f",
   "metadata": {},
   "source": [
    "### Test: Retrieve Documents Related to a Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dd861cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How can I load the redis key name (Document ID) and filter results based on that field?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92436ebc",
   "metadata": {},
   "source": [
    "*K* is the number of documents to retrieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eab9031e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'LOAD hurts the performance of aggregate queries considerably since every processed record</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">needs to execute the equivalent of HMGET against a Redis key, which when executed over millions of keys, amounts to</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">very high processing times.\\n\\n\\nThe document ID can be loaded using @__key.\\n\\n\\n\\nGROUPBY {nargs} {property} ... </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:\\n\\nGroup the results in the pipeline based on one or more properties.\\n\\nEach group should have at least one </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reducer (See below), a function that handles the group entries, either counting them or performing multiple </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">aggregate operations (see below).\\n\\n\\n\\nREDUCE {func} {nargs} {arg} ...\\n\\n[AS {name}]: Reduce the matching </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">results in each group into a single record, using a reduction function.'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'doc:redis-vs-docs:e39c13e594e147c5b8b3929f120e0f65'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'aggregations.md'</span><span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2865</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'For example, if you have an index of car models, with a vehicle class, country of origin,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and engine type, you can search for SUVs made in Korea with hybrid or diesel engines using the following </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">query:\\n\\nFT.SEARCH cars \"@country:korea @engine:(diesel|hybrid) @class:suv\"\\n\\nYou can apply multiple modifiers to</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the same term or grouped terms:\\n\\nFT.SEARCH idx \"@title|body:(hello world)\\n\\n@url|image:mydomain\"\\n\\nNow, you </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">search for documents that have \"hello\" and \"world\" either in the body or the title and the term mydomain in their </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">url or image fields.\\n\\n\\n\\nNumeric filters in query\\n\\nIf a field in the schema is defined as NUMERIC, it is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">possible to use the FILTER argument in the Redis request or filter with it by specifying filtering rules in the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">query.'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'doc:redis-vs-docs:218aaf0983ef4716b2ae87601690d17b'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'query_syntax.md'</span><span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2966</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"For example:\\n\\n```\\n\\nReturn all documents containing all three cities as </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tags\\n\\n@cities:{ New York } @cities:{Los Angeles} @cities:{ Barcelona }\\n\\nNow, return all documents containing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">either city\\n\\n@cities:{ New York | Los Angeles | Barcelona }\\n```\\n\\nTag clauses can be combined into any </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">subclause, used as negative expressions, optional expressions, and so on.\\n\\n\\n\\nGeo filters\\n\\nAs of v0.21, it is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">possible to add geo radius queries directly into the query language with the syntax @field:[{lon} {lat} {radius} </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">{m|km|mi|ft}].\\n\\nThis filters the result to a given radius from a lon,lat point, defined in meters, kilometers, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">miles or feet.\\n\\nSee Redis's own GEORADIUS command for more details.\"</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'doc:redis-vs-docs:91072204a5a445e7b337e2f6e20753ee'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'query_syntax.md'</span><span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3046</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'LOAD hurts the performance of aggregate queries considerably since every processed record\u001b[0m\n",
       "\u001b[32mneeds to execute the equivalent of HMGET against a Redis key, which when executed over millions of keys, amounts to\u001b[0m\n",
       "\u001b[32mvery high processing times.\\n\\n\\nThe document ID can be loaded using @__key.\\n\\n\\n\\nGROUPBY \u001b[0m\u001b[32m{\u001b[0m\u001b[32mnargs\u001b[0m\u001b[32m}\u001b[0m\u001b[32m \u001b[0m\u001b[32m{\u001b[0m\u001b[32mproperty\u001b[0m\u001b[32m}\u001b[0m\u001b[32m ... \u001b[0m\n",
       "\u001b[32m:\\n\\nGroup the results in the pipeline based on one or more properties.\\n\\nEach group should have at least one \u001b[0m\n",
       "\u001b[32mreducer \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSee below\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, a function that handles the group entries, either counting them or performing multiple \u001b[0m\n",
       "\u001b[32maggregate operations \u001b[0m\u001b[32m(\u001b[0m\u001b[32msee below\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n\\n\\nREDUCE \u001b[0m\u001b[32m{\u001b[0m\u001b[32mfunc\u001b[0m\u001b[32m}\u001b[0m\u001b[32m \u001b[0m\u001b[32m{\u001b[0m\u001b[32mnargs\u001b[0m\u001b[32m}\u001b[0m\u001b[32m \u001b[0m\u001b[32m{\u001b[0m\u001b[32marg\u001b[0m\u001b[32m}\u001b[0m\u001b[32m ...\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAS \u001b[0m\u001b[32m{\u001b[0m\u001b[32mname\u001b[0m\u001b[32m}\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: Reduce the matching \u001b[0m\n",
       "\u001b[32mresults in each group into a single record, using a reduction function.'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \u001b[32m'doc:redis-vs-docs:e39c13e594e147c5b8b3929f120e0f65'\u001b[0m, \u001b[32m'source'\u001b[0m: \u001b[32m'aggregations.md'\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;36m0.2865\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1m(\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'For example, if you have an index of car models, with a vehicle class, country of origin,\u001b[0m\n",
       "\u001b[32mand engine type, you can search for SUVs made in Korea with hybrid or diesel engines using the following \u001b[0m\n",
       "\u001b[32mquery:\\n\\nFT.SEARCH cars \"@country:korea @engine:\u001b[0m\u001b[32m(\u001b[0m\u001b[32mdiesel|hybrid\u001b[0m\u001b[32m)\u001b[0m\u001b[32m @class:suv\"\\n\\nYou can apply multiple modifiers to\u001b[0m\n",
       "\u001b[32mthe same term or grouped terms:\\n\\nFT.SEARCH idx \"@title|body:\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhello world\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n@url|image:mydomain\"\\n\\nNow, you \u001b[0m\n",
       "\u001b[32msearch for documents that have \"hello\" and \"world\" either in the body or the title and the term mydomain in their \u001b[0m\n",
       "\u001b[32murl or image fields.\\n\\n\\n\\nNumeric filters in query\\n\\nIf a field in the schema is defined as NUMERIC, it is \u001b[0m\n",
       "\u001b[32mpossible to use the FILTER argument in the Redis request or filter with it by specifying filtering rules in the \u001b[0m\n",
       "\u001b[32mquery.'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \u001b[32m'doc:redis-vs-docs:218aaf0983ef4716b2ae87601690d17b'\u001b[0m, \u001b[32m'source'\u001b[0m: \u001b[32m'query_syntax.md'\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;36m0.2966\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1m(\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m\"For\u001b[0m\u001b[32m example:\\n\\n```\\n\\nReturn all documents containing all three cities as \u001b[0m\n",
       "\u001b[32mtags\\n\\n@cities:\u001b[0m\u001b[32m{\u001b[0m\u001b[32m New York \u001b[0m\u001b[32m}\u001b[0m\u001b[32m @cities:\u001b[0m\u001b[32m{\u001b[0m\u001b[32mLos Angeles\u001b[0m\u001b[32m}\u001b[0m\u001b[32m @cities:\u001b[0m\u001b[32m{\u001b[0m\u001b[32m Barcelona \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\\nNow, return all documents containing \u001b[0m\n",
       "\u001b[32meither city\\n\\n@cities:\u001b[0m\u001b[32m{\u001b[0m\u001b[32m New York | Los Angeles | Barcelona \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n```\\n\\nTag clauses can be combined into any \u001b[0m\n",
       "\u001b[32msubclause, used as negative expressions, optional expressions, and so on.\\n\\n\\n\\nGeo filters\\n\\nAs of v0.21, it is \u001b[0m\n",
       "\u001b[32mpossible to add geo radius queries directly into the query language with the syntax @field:\u001b[0m\u001b[32m[\u001b[0m\u001b[32m{\u001b[0m\u001b[32mlon\u001b[0m\u001b[32m}\u001b[0m\u001b[32m \u001b[0m\u001b[32m{\u001b[0m\u001b[32mlat\u001b[0m\u001b[32m}\u001b[0m\u001b[32m \u001b[0m\u001b[32m{\u001b[0m\u001b[32mradius\u001b[0m\u001b[32m}\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32m{\u001b[0m\u001b[32mm|km|mi|ft\u001b[0m\u001b[32m}\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\n\\nThis filters the result to a given radius from a lon,lat point, defined in meters, kilometers, \u001b[0m\n",
       "\u001b[32mmiles or feet.\\n\\nSee Redis's own GEORADIUS command for more details.\"\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \u001b[32m'doc:redis-vs-docs:91072204a5a445e7b337e2f6e20753ee'\u001b[0m, \u001b[32m'source'\u001b[0m: \u001b[32m'query_syntax.md'\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;36m0.3046\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = vectordb.similarity_search_with_score(question, k=3)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe89eac",
   "metadata": {},
   "source": [
    "A different type of search is Max Marginal Relevance (MMR) search. MMR search is an algorithm that combines the similarity of a document to a query with the similarity of the document to the other documents in the result set. It is useful when you want to retrieve a set of documents that are similar to a query, but also diverse from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1bcb261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'LOAD hurts the performance of aggregate queries considerably since every processed record </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">needs to execute the equivalent of HMGET against a Redis key, which when executed over millions of keys, amounts to</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">very high processing times.\\n\\n\\nThe document ID can be loaded using @__key.\\n\\n\\n\\nGROUPBY {nargs} {property} ... </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:\\n\\nGroup the results in the pipeline based on one or more properties.\\n\\nEach group should have at least one </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reducer (See below), a function that handles the group entries, either counting them or performing multiple </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">aggregate operations (see below).\\n\\n\\n\\nREDUCE {func} {nargs} {arg} ...\\n\\n[AS {name}]: Reduce the matching </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">results in each group into a single record, using a reduction function.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'doc:redis-vs-docs:e39c13e594e147c5b8b3929f120e0f65'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'aggregations.md'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'In DIALECT 2 or greater, this query will be interpreted as \"find James in the @name field AND</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Brown in ANY text field.\\n\\nIn other words, it would be interpreted as (@name:James) Brown.\\n\\n\\n\\nIn DIALECT 2 or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">greater, to achieve the default behavior of DIALECT 1, update your query to @name:(James Brown).\\n\\n\\n\\nIf a field </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">modifier precedes an expression in parentheses, it applies only to the expression inside the parentheses.\\n\\nThe </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">expression should be valid for the specified field, otherwise it is skipped.\\n\\n\\n\\nTo create complex filtering on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">several fields, you can combine multiple modifiers.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'doc:redis-vs-docs:02533342f2ec49cd864eae22c70d56e1'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'query_syntax.md'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Radius filters can be added into the query just like numeric filters.\\n\\nFor example, in a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">database of businesses, looking for Chinese restaurants near San Francisco (within a 5km radius) would be expressed</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">as: chinese restaurant @location:[-122.41 37.77 5 km].\\n\\n\\n\\nPolygon search\\n\\nGeospatial databases are essential </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">for managing and analyzing location-based data in a variety of industries.\\n\\nThey help organizations make </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">data-driven decisions, optimize operations, and achieve their strategic goals more efficiently.\\n\\nPolygon search </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extends Redis\\'s geospatial search capabilities to be able to query against a value in a GEOSHAPE </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">attribute.\\n\\nThis value must follow a \"well-known text\" (WKT) representation of geometry.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'doc:redis-vs-docs:349cc3f4736949e6bc3408f3d55358ed'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'query_syntax.md'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'LOAD hurts the performance of aggregate queries considerably since every processed record \u001b[0m\n",
       "\u001b[32mneeds to execute the equivalent of HMGET against a Redis key, which when executed over millions of keys, amounts to\u001b[0m\n",
       "\u001b[32mvery high processing times.\\n\\n\\nThe document ID can be loaded using @__key.\\n\\n\\n\\nGROUPBY \u001b[0m\u001b[32m{\u001b[0m\u001b[32mnargs\u001b[0m\u001b[32m}\u001b[0m\u001b[32m \u001b[0m\u001b[32m{\u001b[0m\u001b[32mproperty\u001b[0m\u001b[32m}\u001b[0m\u001b[32m ... \u001b[0m\n",
       "\u001b[32m:\\n\\nGroup the results in the pipeline based on one or more properties.\\n\\nEach group should have at least one \u001b[0m\n",
       "\u001b[32mreducer \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSee below\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, a function that handles the group entries, either counting them or performing multiple \u001b[0m\n",
       "\u001b[32maggregate operations \u001b[0m\u001b[32m(\u001b[0m\u001b[32msee below\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n\\n\\nREDUCE \u001b[0m\u001b[32m{\u001b[0m\u001b[32mfunc\u001b[0m\u001b[32m}\u001b[0m\u001b[32m \u001b[0m\u001b[32m{\u001b[0m\u001b[32mnargs\u001b[0m\u001b[32m}\u001b[0m\u001b[32m \u001b[0m\u001b[32m{\u001b[0m\u001b[32marg\u001b[0m\u001b[32m}\u001b[0m\u001b[32m ...\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAS \u001b[0m\u001b[32m{\u001b[0m\u001b[32mname\u001b[0m\u001b[32m}\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: Reduce the matching \u001b[0m\n",
       "\u001b[32mresults in each group into a single record, using a reduction function.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \u001b[32m'doc:redis-vs-docs:e39c13e594e147c5b8b3929f120e0f65'\u001b[0m, \u001b[32m'source'\u001b[0m: \u001b[32m'aggregations.md'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'In DIALECT 2 or greater, this query will be interpreted as \"find James in the @name field AND\u001b[0m\n",
       "\u001b[32mBrown in ANY text field.\\n\\nIn other words, it would be interpreted as \u001b[0m\u001b[32m(\u001b[0m\u001b[32m@name:James\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Brown.\\n\\n\\n\\nIn DIALECT 2 or \u001b[0m\n",
       "\u001b[32mgreater, to achieve the default behavior of DIALECT 1, update your query to @name:\u001b[0m\u001b[32m(\u001b[0m\u001b[32mJames Brown\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n\\n\\nIf a field \u001b[0m\n",
       "\u001b[32mmodifier precedes an expression in parentheses, it applies only to the expression inside the parentheses.\\n\\nThe \u001b[0m\n",
       "\u001b[32mexpression should be valid for the specified field, otherwise it is skipped.\\n\\n\\n\\nTo create complex filtering on \u001b[0m\n",
       "\u001b[32mseveral fields, you can combine multiple modifiers.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \u001b[32m'doc:redis-vs-docs:02533342f2ec49cd864eae22c70d56e1'\u001b[0m, \u001b[32m'source'\u001b[0m: \u001b[32m'query_syntax.md'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'Radius filters can be added into the query just like numeric filters.\\n\\nFor example, in a \u001b[0m\n",
       "\u001b[32mdatabase of businesses, looking for Chinese restaurants near San Francisco \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwithin a 5km radius\u001b[0m\u001b[32m)\u001b[0m\u001b[32m would be expressed\u001b[0m\n",
       "\u001b[32mas: chinese restaurant @location:\u001b[0m\u001b[32m[\u001b[0m\u001b[32m-122.41 37.77 5 km\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\n\\n\\n\\nPolygon search\\n\\nGeospatial databases are essential \u001b[0m\n",
       "\u001b[32mfor managing and analyzing location-based data in a variety of industries.\\n\\nThey help organizations make \u001b[0m\n",
       "\u001b[32mdata-driven decisions, optimize operations, and achieve their strategic goals more efficiently.\\n\\nPolygon search \u001b[0m\n",
       "\u001b[32mextends Redis\\'s geospatial search capabilities to be able to query against a value in a GEOSHAPE \u001b[0m\n",
       "\u001b[32mattribute.\\n\\nThis value must follow a \"well-known text\" \u001b[0m\u001b[32m(\u001b[0m\u001b[32mWKT\u001b[0m\u001b[32m)\u001b[0m\u001b[32m representation of geometry.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \u001b[32m'doc:redis-vs-docs:349cc3f4736949e6bc3408f3d55358ed'\u001b[0m, \u001b[32m'source'\u001b[0m: \u001b[32m'query_syntax.md'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = vectordb.max_marginal_relevance_search(question, k=3, top_k=5, threshold=0.5)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f307cf",
   "metadata": {},
   "source": [
    "## 1. Semantic Search - Question Answering (Q&A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54870447",
   "metadata": {},
   "source": [
    "We will create a prommpt template that will provide instructions to the LLM,\n",
    "as well as contain placeholders for the context (retrieved from Redis) and the question (asked by the user)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80dccb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "QA_TEMPLATE = \"\"\"\n",
    "Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum. Keep the answer as concise as possible. \n",
    "-----\n",
    "Context: \n",
    "\n",
    "{context}\n",
    "-----\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], \n",
    "    template=QA_TEMPLATE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b4b3a4",
   "metadata": {},
   "source": [
    "Next we will create an LLM object and use it to generate answers to our questions.\n",
    "\n",
    "We will also create the `RetrievalQA` chain, which will retrieve the most relevant documents from Redis, and use them as context for the LLM.\n",
    "\n",
    "We are specifying:\n",
    "* The LLM model name (`gemini-pro`)\n",
    "* The maximum length of the generated answer (`max_output_tokens`)\n",
    "* The LLM temperature (`temperature`), which controls the randomness of the generated text. Higher values will result in more random text while lower values will result in more predictable text.\n",
    "\n",
    "The type of chain we're creating is a `stuff` chain, as in \"stuff the retrieved documents into the LLM\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "beb248a1-f56d-4dde-9f8a-ad32a5089f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import VertexAI\n",
    "\n",
    "# Define LLM to generate response\n",
    "llm = VertexAI(model_name='gemini-pro', max_output_tokens=512, temperature=0.3)\n",
    "\n",
    "# Create QA chain to respond to user query along with source documents\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bf0aa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How can I load the redis key name (Document ID) and filter results based on that field?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67414866-4785-4488-b5e5-fb9cbef5e691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "score_threshold is deprecated. Use distance_threshold instead.score_threshold should only be used in similarity_search_with_relevance_scores.score_threshold will be removed in a future release.\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">To load the redis key name <span style=\"font-weight: bold\">(</span>Document ID<span style=\"font-weight: bold\">)</span> and filter results based on that field, you can use the following steps:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **Load the document ID using `@__key`**: This will load the document ID for each record in the result set.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. **Use the `FILTER` argument with the `@__key` field**: This will allow you to filter the results based on the \n",
       "document ID.\n",
       "\n",
       "Here is an example query that demonstrates how to do this:\n",
       "\n",
       "```\n",
       "FT.SEARCH my_index <span style=\"color: #008000; text-decoration-color: #008000\">\"@__key:{my_key}\"</span>\n",
       "```\n",
       "\n",
       "This query will return all documents in the `my_index` index that have a document ID of `my_key`.\n",
       "\n",
       "You can also use the `FILTER` argument with other operators to filter the results based on the document ID. For \n",
       "example, the following query will return all documents that have a document ID that starts with `my_key`:\n",
       "\n",
       "```\n",
       "FT.SEARCH my_index <span style=\"color: #008000; text-decoration-color: #008000\">\"@__key:{my_key*}\"</span>\n",
       "```\n",
       "\n",
       "I hope this helps!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "To load the redis key name \u001b[1m(\u001b[0mDocument ID\u001b[1m)\u001b[0m and filter results based on that field, you can use the following steps:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. **Load the document ID using `@__key`**: This will load the document ID for each record in the result set.\n",
       "\u001b[1;36m2\u001b[0m. **Use the `FILTER` argument with the `@__key` field**: This will allow you to filter the results based on the \n",
       "document ID.\n",
       "\n",
       "Here is an example query that demonstrates how to do this:\n",
       "\n",
       "```\n",
       "FT.SEARCH my_index \u001b[32m\"@__key:\u001b[0m\u001b[32m{\u001b[0m\u001b[32mmy_key\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\"\u001b[0m\n",
       "```\n",
       "\n",
       "This query will return all documents in the `my_index` index that have a document ID of `my_key`.\n",
       "\n",
       "You can also use the `FILTER` argument with other operators to filter the results based on the document ID. For \n",
       "example, the following query will return all documents that have a document ID that starts with `my_key`:\n",
       "\n",
       "```\n",
       "FT.SEARCH my_index \u001b[32m\"@__key:\u001b[0m\u001b[32m{\u001b[0m\u001b[32mmy_key*\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\"\u001b[0m\n",
       "```\n",
       "\n",
       "I hope this helps!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.6 ms, sys: 26.2 ms, total: 75.8 ms\n",
      "Wall time: 3.01 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run QA chain\n",
    "result = qa({\"query\": question})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aff2925",
   "metadata": {},
   "source": [
    "## 2. Semantic Cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1094323",
   "metadata": {},
   "source": [
    "Making calls to a (paid) LLM API can get very expensive, very quickly. We can use Redis to cache the results of our LLM calls, and use the cache to answer questions that we've already answered before.\n",
    "\n",
    "This will not only save on API usage costs, but will also significantly speed up our response times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e471f9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.cache import RedisSemanticCache\n",
    "\n",
    "langchain.llm_cache = RedisSemanticCache(\n",
    "    embedding=embedding,\n",
    "    redis_url=\"redis://localhost:6379\",\n",
    "    score_threshold=0.2  # what is the maximum distance between the query and the retrieved document\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40009aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How do I get documents withing a certain radius from a point?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e33c66cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score_threshold is deprecated. Use distance_threshold instead.score_threshold should only be used in similarity_search_with_relevance_scores.score_threshold will be removed in a future release.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">You can use the `@field:<span style=\"font-weight: bold\">[{</span>lon<span style=\"font-weight: bold\">}</span> <span style=\"font-weight: bold\">{</span>lat<span style=\"font-weight: bold\">}</span> <span style=\"font-weight: bold\">{</span>radius<span style=\"font-weight: bold\">}</span> <span style=\"font-weight: bold\">{</span>m|km|mi|ft<span style=\"font-weight: bold\">}]</span>` syntax to filter documents within a certain radius \n",
       "from a point. For example, to find Chinese restaurants near San Francisco <span style=\"font-weight: bold\">(</span>within a 5km radius<span style=\"font-weight: bold\">)</span>, you would use the \n",
       "following query:\n",
       "\n",
       "```\n",
       "chinese restaurant @location:<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-122.41</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">37.77</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> km<span style=\"font-weight: bold\">]</span>\n",
       "```\n",
       "</pre>\n"
      ],
      "text/plain": [
       "You can use the `@field:\u001b[1m[\u001b[0m\u001b[1m{\u001b[0mlon\u001b[1m}\u001b[0m \u001b[1m{\u001b[0mlat\u001b[1m}\u001b[0m \u001b[1m{\u001b[0mradius\u001b[1m}\u001b[0m \u001b[1m{\u001b[0mm|km|mi|ft\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m` syntax to filter documents within a certain radius \n",
       "from a point. For example, to find Chinese restaurants near San Francisco \u001b[1m(\u001b[0mwithin a 5km radius\u001b[1m)\u001b[0m, you would use the \n",
       "following query:\n",
       "\n",
       "```\n",
       "chinese restaurant @location:\u001b[1m[\u001b[0m\u001b[1;36m-122.41\u001b[0m \u001b[1;36m37.77\u001b[0m \u001b[1;36m5\u001b[0m km\u001b[1m]\u001b[0m\n",
       "```\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.5 ms, sys: 12.6 ms, total: 54.1 ms\n",
      "Wall time: 2.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = qa({\"query\": question})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e36d5031",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How do I get documents withing a certain radius from a coordinate?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9f6f8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score_threshold is deprecated. Use distance_threshold instead.score_threshold should only be used in similarity_search_with_relevance_scores.score_threshold will be removed in a future release.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">You can use the `@field:<span style=\"font-weight: bold\">[{</span>lon<span style=\"font-weight: bold\">}</span> <span style=\"font-weight: bold\">{</span>lat<span style=\"font-weight: bold\">}</span> <span style=\"font-weight: bold\">{</span>radius<span style=\"font-weight: bold\">}</span> <span style=\"font-weight: bold\">{</span>m|km|mi|ft<span style=\"font-weight: bold\">}]</span>` syntax to filter documents within a certain radius \n",
       "from a point. For example, to find Chinese restaurants near San Francisco <span style=\"font-weight: bold\">(</span>within a 5km radius<span style=\"font-weight: bold\">)</span>, you would use the \n",
       "following query:\n",
       "\n",
       "```\n",
       "chinese restaurant @location:<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-122.41</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">37.77</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> km<span style=\"font-weight: bold\">]</span>\n",
       "```\n",
       "</pre>\n"
      ],
      "text/plain": [
       "You can use the `@field:\u001b[1m[\u001b[0m\u001b[1m{\u001b[0mlon\u001b[1m}\u001b[0m \u001b[1m{\u001b[0mlat\u001b[1m}\u001b[0m \u001b[1m{\u001b[0mradius\u001b[1m}\u001b[0m \u001b[1m{\u001b[0mm|km|mi|ft\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m` syntax to filter documents within a certain radius \n",
       "from a point. For example, to find Chinese restaurants near San Francisco \u001b[1m(\u001b[0mwithin a 5km radius\u001b[1m)\u001b[0m, you would use the \n",
       "following query:\n",
       "\n",
       "```\n",
       "chinese restaurant @location:\u001b[1m[\u001b[0m\u001b[1;36m-122.41\u001b[0m \u001b[1;36m37.77\u001b[0m \u001b[1;36m5\u001b[0m km\u001b[1m]\u001b[0m\n",
       "```\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.4 ms, sys: 13.5 ms, total: 44.9 ms\n",
      "Wall time: 439 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "result = qa({\"query\": question})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a736d28b",
   "metadata": {},
   "source": [
    "## 3. Chat Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028c3563",
   "metadata": {},
   "source": [
    "In this use case, we'll use Redis to provide a memory to our chatbot. We'll use the memory to store the user's questions and the LLM's answers, and use them to provide context to the LLM in subsequent questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3c95687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "\n",
    "# Clear cache\n",
    "langchain.llm_cache = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7323976c",
   "metadata": {},
   "source": [
    "## I Do Not Recall\n",
    "\n",
    "First, let's have a chat with the LLM ***without*** any memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9832e65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import VertexAI\n",
    "\n",
    "# Define LLM to generate response\n",
    "llm = VertexAI(model_name='gemini-pro', max_output_tokens=512, temperature=0.2)\n",
    "\n",
    "template = \"\"\"You are an assistant designed to be able to assist with a wide range of tasks, \n",
    "from answering simple questions to providing in-depth explanations and discussions on a wide range of topics.\n",
    "\n",
    "Human: {human_input}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"human_input\"], \n",
    "    template=template\n",
    "    )\n",
    "\n",
    "# Create QA chain to respond to user query along with source documents\n",
    "chat = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd20009",
   "metadata": {},
   "source": [
    "Using `verbose=True`, we can see the LLM's context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e39ff809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant designed to be able to assist with a wide range of tasks, \n",
      "from answering simple questions to providing in-depth explanations and discussions on a wide range of topics.\n",
      "\n",
      "Human: Hi, my name is Eli. I like eating noodles and I work at Redis. What is your name?\n",
      "Assistant:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Hi Eli, I'm Bard. It's nice to meet you! I'm glad to hear you enjoy eating noodles. I'm not familiar with Redis, \n",
       "but I'm always happy to learn new things. What kind of work do you do there?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Hi Eli, I'm Bard. It's nice to meet you! I'm glad to hear you enjoy eating noodles. I'm not familiar with Redis, \n",
       "but I'm always happy to learn new things. What kind of work do you do there?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reply = chat.predict(human_input=\"Hi, my name is Eli. I like eating noodles and I work at Redis. What is your name?\")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54922686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant designed to be able to assist with a wide range of tasks, \n",
      "from answering simple questions to providing in-depth explanations and discussions on a wide range of topics.\n",
      "\n",
      "Human: Who won the World Cup in 2018?\n",
      "Assistant:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2018</span> FIFA World Cup was won by France, who defeated Croatia <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> in the final. This was France's second World \n",
       "Cup title, their first since <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1998</span>. The tournament was held in Russia from June <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span> to July <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2018</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The \u001b[1;36m2018\u001b[0m FIFA World Cup was won by France, who defeated Croatia \u001b[1;36m4\u001b[0m-\u001b[1;36m2\u001b[0m in the final. This was France's second World \n",
       "Cup title, their first since \u001b[1;36m1998\u001b[0m. The tournament was held in Russia from June \u001b[1;36m14\u001b[0m to July \u001b[1;36m15\u001b[0m, \u001b[1;36m2018\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reply = chat.predict(human_input=\"Who won the World Cup in 2018?\")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec658166",
   "metadata": {},
   "source": [
    "If we had memory, the LLM would know the answer to the next question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2310d2a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant designed to be able to assist with a wide range of tasks, \n",
      "from answering simple questions to providing in-depth explanations and discussions on a wide range of topics.\n",
      "\n",
      "Human: What's my name?\n",
      "Assistant:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Human: What's my name?\n",
       "Assistant: I'm sorry, but I don't have access to your personal information, including your name. Is there anything \n",
       "else I can help you with?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Human: What's my name?\n",
       "Assistant: I'm sorry, but I don't have access to your personal information, including your name. Is there anything \n",
       "else I can help you with?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reply = chat.predict(human_input=\"What's my name?\")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1e61d9",
   "metadata": {},
   "source": [
    "---\n",
    "## Total Recall\n",
    "Now let's build the same chatbot ***with*** memory.\n",
    "\n",
    "The message history will be stored in Redis, and the LLM will use it to provide context to the next question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "807c7c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import RedisChatMessageHistory, ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Define LLM to generate response\n",
    "llm = VertexAI(model_name='gemini-pro', max_output_tokens=512, temperature=0.2)\n",
    "\n",
    "template = \"\"\"You are an assistant designed to be able to assist with a wide range of tasks, \n",
    "from answering simple questions to providing in-depth explanations and discussions on a wide range of topics.\n",
    "\n",
    "Use the human input to generate a response that is relevant to the conversation history.\n",
    "----------\n",
    "History:\n",
    "\n",
    "{history}\n",
    "----------\n",
    "Human: {human_input}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"human_input\"], \n",
    "    template=template\n",
    "    )\n",
    "\n",
    "# define the chat message memory\n",
    "message_history = RedisChatMessageHistory(key_prefix=\"chat-history:\", session_id=\"vs-demo\")\n",
    "message_history.clear()\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"history\", chat_memory=message_history\n",
    ")\n",
    "\n",
    "# Create QA chain to respond to user query along with source documents\n",
    "chat = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0706d1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant designed to be able to assist with a wide range of tasks, \n",
      "from answering simple questions to providing in-depth explanations and discussions on a wide range of topics.\n",
      "\n",
      "Use the human input to generate a response that is relevant to the conversation history.\n",
      "----------\n",
      "History:\n",
      "\n",
      "\n",
      "----------\n",
      "Human: Hi, my name is Adam. I have 3 kids and I like gardening. What is your name?\n",
      "Assistant:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Hi Adam, it's nice to meet you. I'm Gemini, a large language model from Google AI. I'm glad to hear you enjoy \n",
       "gardening. It's a great way to relax and connect with nature. Do you have any favorite plants or vegetables that \n",
       "you grow?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Hi Adam, it's nice to meet you. I'm Gemini, a large language model from Google AI. I'm glad to hear you enjoy \n",
       "gardening. It's a great way to relax and connect with nature. Do you have any favorite plants or vegetables that \n",
       "you grow?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reply = chat.predict(human_input=\"Hi, my name is Adam. I have 3 kids and I like gardening. What is your name?\")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd12418b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant designed to be able to assist with a wide range of tasks, \n",
      "from answering simple questions to providing in-depth explanations and discussions on a wide range of topics.\n",
      "\n",
      "Use the human input to generate a response that is relevant to the conversation history.\n",
      "----------\n",
      "History:\n",
      "\n",
      "Human: Hi, my name is Adam. I have 3 kids and I like gardening. What is your name?\n",
      "AI: Hi Adam, it's nice to meet you. I'm Gemini, a large language model from Google AI. I'm glad to hear you enjoy gardening. It's a great way to relax and connect with nature. Do you have any favorite plants or vegetables that you grow?\n",
      "----------\n",
      "Human: How long was the last Harry Potter book?\n",
      "Assistant:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The last Harry Potter book, <span style=\"color: #008000; text-decoration-color: #008000\">\"Harry Potter and the Deathly Hallows,\"</span> was published in two volumes. The first volume \n",
       "was released on July <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2007</span>, and the second volume was released on July <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2007</span>. The first volume is <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">607</span> pages \n",
       "long, and the second volume is <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">652</span> pages long. \n",
       "\n",
       "The book follows Harry Potter and his friends as they search for the remaining Horcruxes, pieces of Voldemort's \n",
       "soul that must be destroyed in order to defeat him. The book culminates in a final battle between Harry and \n",
       "Voldemort at Hogwarts School of Witchcraft and Wizardry.\n",
       "\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"Harry Potter and the Deathly Hallows\"</span> was a critical and commercial success, selling over <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span> million copies in its\n",
       "first <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span> hours of release. It was the fastest-selling book in history at the time, and it remains one of the \n",
       "best-selling books of all time.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The last Harry Potter book, \u001b[32m\"Harry Potter and the Deathly Hallows,\"\u001b[0m was published in two volumes. The first volume \n",
       "was released on July \u001b[1;36m21\u001b[0m, \u001b[1;36m2007\u001b[0m, and the second volume was released on July \u001b[1;36m21\u001b[0m, \u001b[1;36m2007\u001b[0m. The first volume is \u001b[1;36m607\u001b[0m pages \n",
       "long, and the second volume is \u001b[1;36m652\u001b[0m pages long. \n",
       "\n",
       "The book follows Harry Potter and his friends as they search for the remaining Horcruxes, pieces of Voldemort's \n",
       "soul that must be destroyed in order to defeat him. The book culminates in a final battle between Harry and \n",
       "Voldemort at Hogwarts School of Witchcraft and Wizardry.\n",
       "\n",
       "\u001b[32m\"Harry Potter and the Deathly Hallows\"\u001b[0m was a critical and commercial success, selling over \u001b[1;36m15\u001b[0m million copies in its\n",
       "first \u001b[1;36m24\u001b[0m hours of release. It was the fastest-selling book in history at the time, and it remains one of the \n",
       "best-selling books of all time.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reply = chat.predict(human_input=\"How long was the last Harry Potter book?\")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8a50c708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant designed to be able to assist with a wide range of tasks, \n",
      "from answering simple questions to providing in-depth explanations and discussions on a wide range of topics.\n",
      "\n",
      "Use the human input to generate a response that is relevant to the conversation history.\n",
      "----------\n",
      "History:\n",
      "\n",
      "Human: Hi, my name is Adam. I have 3 kids and I like gardening. What is your name?\n",
      "AI: Hi Adam, it's nice to meet you. I'm Gemini, a large language model from Google AI. I'm glad to hear you enjoy gardening. It's a great way to relax and connect with nature. Do you have any favorite plants or vegetables that you grow?\n",
      "Human: How long was the last Harry Potter book?\n",
      "AI: The last Harry Potter book, \"Harry Potter and the Deathly Hallows,\" was published in two volumes. The first volume was released on July 21, 2007, and the second volume was released on July 21, 2007. The first volume is 607 pages long, and the second volume is 652 pages long. \n",
      "\n",
      "The book follows Harry Potter and his friends as they search for the remaining Horcruxes, pieces of Voldemort's soul that must be destroyed in order to defeat him. The book culminates in a final battle between Harry and Voldemort at Hogwarts School of Witchcraft and Wizardry.\n",
      "\n",
      "\"Harry Potter and the Deathly Hallows\" was a critical and commercial success, selling over 15 million copies in its first 24 hours of release. It was the fastest-selling book in history at the time, and it remains one of the best-selling books of all time.\n",
      "----------\n",
      "Human: What's the name of their school?\n",
      "Assistant:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The school in the Harry Potter series is called Hogwarts School of Witchcraft and Wizardry. It is located in \n",
       "Scotland and is the premier school for magic in the wizarding world. \n",
       "</pre>\n"
      ],
      "text/plain": [
       "The school in the Harry Potter series is called Hogwarts School of Witchcraft and Wizardry. It is located in \n",
       "Scotland and is the premier school for magic in the wizarding world. \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reply = chat.predict(human_input=\"What's the name of their school?\")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c6c6e6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant designed to be able to assist with a wide range of tasks, \n",
      "from answering simple questions to providing in-depth explanations and discussions on a wide range of topics.\n",
      "\n",
      "Use the human input to generate a response that is relevant to the conversation history.\n",
      "----------\n",
      "History:\n",
      "\n",
      "Human: Hi, my name is Adam. I have 3 kids and I like gardening. What is your name?\n",
      "AI: Hi Adam, it's nice to meet you. I'm Gemini, a large language model from Google AI. I'm glad to hear you enjoy gardening. It's a great way to relax and connect with nature. Do you have any favorite plants or vegetables that you grow?\n",
      "Human: How long was the last Harry Potter book?\n",
      "AI: The last Harry Potter book, \"Harry Potter and the Deathly Hallows,\" was published in two volumes. The first volume was released on July 21, 2007, and the second volume was released on July 21, 2007. The first volume is 607 pages long, and the second volume is 652 pages long. \n",
      "\n",
      "The book follows Harry Potter and his friends as they search for the remaining Horcruxes, pieces of Voldemort's soul that must be destroyed in order to defeat him. The book culminates in a final battle between Harry and Voldemort at Hogwarts School of Witchcraft and Wizardry.\n",
      "\n",
      "\"Harry Potter and the Deathly Hallows\" was a critical and commercial success, selling over 15 million copies in its first 24 hours of release. It was the fastest-selling book in history at the time, and it remains one of the best-selling books of all time.\n",
      "Human: What's the name of their school?\n",
      "AI: The school in the Harry Potter series is called Hogwarts School of Witchcraft and Wizardry. It is located in Scotland and is the premier school for magic in the wizarding world. \n",
      "----------\n",
      "Human: What train platform was the train on?\n",
      "Assistant:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The train that Harry and his friends take to Hogwarts departs from Platform <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> at King's Cross Station in \n",
       "London. This platform is hidden from Muggles <span style=\"font-weight: bold\">(</span>non-magical people<span style=\"font-weight: bold\">)</span> and can only be accessed by witches and wizards. \n",
       "To reach the platform, one must walk straight through the barrier between platforms <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span> and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>. \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The train that Harry and his friends take to Hogwarts departs from Platform \u001b[1;36m9\u001b[0m \u001b[1;36m3\u001b[0m/\u001b[1;36m4\u001b[0m at King's Cross Station in \n",
       "London. This platform is hidden from Muggles \u001b[1m(\u001b[0mnon-magical people\u001b[1m)\u001b[0m and can only be accessed by witches and wizards. \n",
       "To reach the platform, one must walk straight through the barrier between platforms \u001b[1;36m9\u001b[0m and \u001b[1;36m10\u001b[0m. \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reply = chat.predict(human_input=\"What train platform was the train on?\")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d2dfb571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant designed to be able to assist with a wide range of tasks, \n",
      "from answering simple questions to providing in-depth explanations and discussions on a wide range of topics.\n",
      "\n",
      "Use the human input to generate a response that is relevant to the conversation history.\n",
      "----------\n",
      "History:\n",
      "\n",
      "Human: Hi, my name is Adam. I have 3 kids and I like gardening. What is your name?\n",
      "AI: Hi Adam, it's nice to meet you. I'm Gemini, a large language model from Google AI. I'm glad to hear you enjoy gardening. It's a great way to relax and connect with nature. Do you have any favorite plants or vegetables that you grow?\n",
      "Human: How long was the last Harry Potter book?\n",
      "AI: The last Harry Potter book, \"Harry Potter and the Deathly Hallows,\" was published in two volumes. The first volume was released on July 21, 2007, and the second volume was released on July 21, 2007. The first volume is 607 pages long, and the second volume is 652 pages long. \n",
      "\n",
      "The book follows Harry Potter and his friends as they search for the remaining Horcruxes, pieces of Voldemort's soul that must be destroyed in order to defeat him. The book culminates in a final battle between Harry and Voldemort at Hogwarts School of Witchcraft and Wizardry.\n",
      "\n",
      "\"Harry Potter and the Deathly Hallows\" was a critical and commercial success, selling over 15 million copies in its first 24 hours of release. It was the fastest-selling book in history at the time, and it remains one of the best-selling books of all time.\n",
      "Human: What's the name of their school?\n",
      "AI: The school in the Harry Potter series is called Hogwarts School of Witchcraft and Wizardry. It is located in Scotland and is the premier school for magic in the wizarding world. \n",
      "Human: What train platform was the train on?\n",
      "AI: The train that Harry and his friends take to Hogwarts departs from Platform 9 3/4 at King's Cross Station in London. This platform is hidden from Muggles (non-magical people) and can only be accessed by witches and wizards. To reach the platform, one must walk straight through the barrier between platforms 9 and 10. \n",
      "\n",
      "----------\n",
      "Human: Do you remember my name?\n",
      "Assistant:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Yes, I remember your name is Adam. You have <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> kids and you enjoy gardening. You also asked me about the length of \n",
       "the last Harry Potter book, the name of their school, and the train platform the train was on. Is there anything \n",
       "else I can help you with today?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Yes, I remember your name is Adam. You have \u001b[1;36m3\u001b[0m kids and you enjoy gardening. You also asked me about the length of \n",
       "the last Harry Potter book, the name of their school, and the train platform the train was on. Is there anything \n",
       "else I can help you with today?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reply = chat.predict(human_input=\"Do you remember my name?\")\n",
    "print(reply)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
