{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "209612cf",
   "metadata": {},
   "source": [
    "# LLM Applications With Redis Enterprise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32b2b86",
   "metadata": {},
   "source": [
    "In this demo we'll show 3 common use cases for Redis Enterprise in LLM applications:\n",
    "1. **Semantic Search** (i.e., Vector Search), and **RAG (Retrieval-Augmented Generation)** to chat with a knowledge base\n",
    "2. **Semantic Cache**\n",
    "3. **Chat Memory**\n",
    "\n",
    "We'll use [LangChain](https://www.langchain.com/) to compose these use cases. You can sign up for a free Redis database [here](https://redis.com/try-free/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f80ac14",
   "metadata": {},
   "source": [
    "The diagram below shows the demo architecture.\n",
    "\n",
    "![](vss-vw-demo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b3e3e0",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc70abe-fa70-4800-b58e-42174e7641d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install redis langchain rich spacy google-cloud-aiplatform unstructured markdown python-dotenv requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5560214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faae6be4",
   "metadata": {},
   "source": [
    "Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c7a4eb-b703-4880-8553-d8e57b8b15e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print # this will pretty-print python objects\n",
    "import warnings\n",
    "import dotenv\n",
    "\n",
    "# mute warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# load env vars from .env file\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "def download_file(url, filename):\n",
    "    import requests\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open(filename, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610c9d24",
   "metadata": {},
   "source": [
    "## 0. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0b373c",
   "metadata": {},
   "source": [
    "### Load Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bca13ad",
   "metadata": {},
   "source": [
    "Let's talk to the Redis documentation. We'll load a local copy of the Search [Aggregations](https://redis.io/docs/interact/search-and-query/search/aggregations/) and [Query](https://redis.io/docs/interact/search-and-query/query/) pages and use them to answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7235d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "# download aggregation doc\n",
    "aggs_url = \"https://github.com/RediSearch/RediSearch/raw/master/docs/docs/advanced-concepts/aggregations.md\"\n",
    "aggs_doc_path = \"aggregations.md\"\n",
    "download_file(aggs_url, aggs_doc_path)\n",
    "docs = UnstructuredMarkdownLoader(aggs_doc_path).load()\n",
    "\n",
    "# download query syntax doc\n",
    "query_syntax_url = \"https://github.com/RediSearch/RediSearch/raw/master/docs/docs/advanced-concepts/query_syntax.md\"\n",
    "query_doc_path = \"query_syntax.md\"\n",
    "download_file(query_syntax_url, query_doc_path)\n",
    "docs.extend(UnstructuredMarkdownLoader(query_doc_path).load())\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3c5570",
   "metadata": {},
   "source": [
    "### Split Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e16c4d4",
   "metadata": {},
   "source": [
    "Next, we'll split the doument into chunks and index each chunk as a separate document.\n",
    "\n",
    "This will allow us to retrieve specific, smaller, relevant chunks of the document to add context to our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2cad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents into chunks\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "text_splitter = SpacyTextSplitter(chunk_size=750, chunk_overlap=50, strip_whitespace=True)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "print(f\"Generated {len(splits)} splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f242c",
   "metadata": {},
   "source": [
    "### Create Embeddings, Load Into Redis and Create Search Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeb1f0f",
   "metadata": {},
   "source": [
    "Let's create our embeddings transfromer. We will use it to transform our documents, the user's questions, and our prompts into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5830869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "\n",
    "# Define Text Embeddings model\n",
    "embedding = VertexAIEmbeddings(model_name=\"textembedding-gecko\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdb9f7c",
   "metadata": {},
   "source": [
    "We can now use the embeddings object to transform our documents content, then load the documents into Redis.\n",
    "\n",
    "This step will also create a search index called `redis-vs-docs` on the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62489946",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create embeddings and load data into Redis\n",
    "from langchain.vectorstores import Redis\n",
    "\n",
    "vectordb = Redis.from_documents(documents=splits, embedding=embedding, index_name=\"redis-vs-docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7792474f",
   "metadata": {},
   "source": [
    "### Test: Retrieve Documents Related to a Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd861cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How can I load the redis key name (Document ID) and filter results based on that field?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92436ebc",
   "metadata": {},
   "source": [
    "*K* is the number of documents to retrieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab9031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vectordb.similarity_search_with_score(question, k=3)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe89eac",
   "metadata": {},
   "source": [
    "A different type of search is Max Marginal Relevance (MMR) search. MMR search is an algorithm that combines the similarity of a document to a query with the similarity of the document to the other documents in the result set. It is useful when you want to retrieve a set of documents that are similar to a query, but also diverse from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bcb261",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vectordb.max_marginal_relevance_search(question, k=3, top_k=5, threshold=0.5)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f307cf",
   "metadata": {},
   "source": [
    "## 1. Semantic Search - Question Answering (Q&A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54870447",
   "metadata": {},
   "source": [
    "We will create a prommpt template that will provide instructions to the LLM,\n",
    "as well as contain placeholders for the context (retrieved from Redis) and the question (asked by the user)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dccb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "QA_TEMPLATE = \"\"\"\n",
    "Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum. Keep the answer as concise as possible. \n",
    "-----\n",
    "Context: \n",
    "\n",
    "{context}\n",
    "-----\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], \n",
    "    template=QA_TEMPLATE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b4b3a4",
   "metadata": {},
   "source": [
    "Next we will create an LLM object and use it to generate answers to our questions.\n",
    "\n",
    "We will also create the `RetrievalQA` chain, which will retrieve the most relevant documents from Redis, and use them as context for the LLM.\n",
    "\n",
    "We are specifying:\n",
    "* The LLM model name (`gemini-pro`)\n",
    "* The maximum length of the generated answer (`max_output_tokens`)\n",
    "* The LLM temperature (`temperature`), which controls the randomness of the generated text. Higher values will result in more random text while lower values will result in more predictable text.\n",
    "\n",
    "The type of chain we're creating is a `stuff` chain, as in \"stuff the retrieved documents into the LLM\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb248a1-f56d-4dde-9f8a-ad32a5089f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_google_vertexai import VertexAI\n",
    "\n",
    "# Define LLM to generate response\n",
    "llm = VertexAI(model_name='gemini-pro', max_output_tokens=512, temperature=0.3)\n",
    "\n",
    "# Create QA chain to respond to user query along with source documents\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf0aa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How can I load the redis key name (Document ID) and filter results based on that field?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67414866-4785-4488-b5e5-fb9cbef5e691",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run QA chain\n",
    "result = qa({\"query\": question})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aff2925",
   "metadata": {},
   "source": [
    "## 2. Semantic Cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1094323",
   "metadata": {},
   "source": [
    "Making calls to a (paid) LLM API can get very expensive, very quickly. We can use Redis to cache the results of our LLM calls, and use the cache to answer questions that we've already answered before.\n",
    "\n",
    "This will not only save on API usage costs, but will also significantly speed up our response times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e471f9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.cache import RedisSemanticCache\n",
    "\n",
    "langchain.llm_cache = RedisSemanticCache(\n",
    "    embedding=embedding,\n",
    "    redis_url=\"redis://localhost:6379\",\n",
    "    score_threshold=0.1  # what is the maximum distance between the query and the retrieved document\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40009aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How do I get documents withing a certain radius from a point?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33c66cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = qa({\"query\": question})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36d5031",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How do I get documents withing a certain radius from a coordinate?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f6f8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "result = qa({\"query\": question})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a736d28b",
   "metadata": {},
   "source": [
    "## 3. Chat Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028c3563",
   "metadata": {},
   "source": [
    "In this use case, we'll use Redis to provide a memory to our chatbot. We'll use the memory to store the user's questions and the LLM's answers, and use them to provide context to the LLM in subsequent questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c95687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "\n",
    "# Clear cache\n",
    "langchain.llm_cache = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7323976c",
   "metadata": {},
   "source": [
    "## I Do Not Recall\n",
    "\n",
    "First, let's have a chat with the LLM ***without*** any memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9832e65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_vertexai import VertexAI\n",
    "\n",
    "# Define LLM to generate response\n",
    "llm = VertexAI(model_name='gemini-pro', max_output_tokens=512, temperature=0.2)\n",
    "\n",
    "template = \"\"\"You are an assistant designed to be able to assist with a wide range of tasks, \n",
    "from answering simple questions to providing in-depth explanations and discussions on a wide range of topics.\n",
    "\n",
    "Human: {human_input}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"human_input\"], \n",
    "    template=template\n",
    "    )\n",
    "\n",
    "# Create QA chain to respond to user query along with source documents\n",
    "chat = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd20009",
   "metadata": {},
   "source": [
    "Using `verbose=True`, we can see the LLM's context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39ff809",
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = chat.predict(human_input=\"Hi, my name is Eli. I like eating noodles and I work at Redis. What is your name?\")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54922686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reply = chat.predict(human_input=\"Who won the World Cup in 2018?\")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec658166",
   "metadata": {},
   "source": [
    "If we had memory, the LLM would know the answer to the next question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2310d2a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reply = chat.predict(human_input=\"What's my name?\")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1e61d9",
   "metadata": {},
   "source": [
    "---\n",
    "## Total Recall\n",
    "Now let's build the same chatbot ***with*** memory.\n",
    "\n",
    "The message history will be stored in Redis, and the LLM will use it to provide context to the next question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807c7c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import RedisChatMessageHistory, ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_vertexai import VertexAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Define LLM to generate response\n",
    "llm = VertexAI(model_name='gemini-pro', max_output_tokens=512, temperature=0.3)\n",
    "\n",
    "template = \"\"\"You are an assistant designed to be able to assist with a wide range of tasks, \n",
    "from answering simple questions to providing in-depth explanations and discussions on a wide range of topics.\n",
    "\n",
    "Use the human input to generate a response that is relevant to the conversation history.\n",
    "----------\n",
    "History:\n",
    "\n",
    "{history}\n",
    "----------\n",
    "Human: {human_input}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"human_input\"], \n",
    "    template=template\n",
    "    )\n",
    "\n",
    "# define the chat message memory\n",
    "message_history = RedisChatMessageHistory(key_prefix=\"chat-history:\", session_id=\"vs-demo\")\n",
    "message_history.clear()\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"history\", chat_memory=message_history\n",
    ")\n",
    "\n",
    "# Create QA chain to respond to user query along with source documents\n",
    "chat = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0706d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = chat.predict(human_input=\"Hi, my name is Adam. I have 3 kids and I like gardening. What is your name?\")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd12418b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = chat.predict(human_input=\"How long was the last Harry Potter book?\")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a50c708",
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = chat.predict(human_input=\"What's the name of their school?\")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c6e6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = chat.predict(human_input=\"What train platform was the train on?\")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dfb571",
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = chat.predict(human_input=\"Do you remember my name?\")\n",
    "print(reply)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
