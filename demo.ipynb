{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "209612cf",
   "metadata": {},
   "source": [
    "# LLM Applications With Redis Enterprise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32b2b86",
   "metadata": {},
   "source": [
    "In this demo we'll show 3 common use cases for Redis Enterprise in LLM applications:\n",
    "1. **Semantic Search** (i.e., Vector Search), and **RAG (Retrieval-Augmented Generation)** to chat with a knowledge base\n",
    "2. **Semantic Cache**\n",
    "3. **Chat Memory**\n",
    "\n",
    "We'll use [LangChain](https://www.langchain.com/) to compose these use cases. You can sign up for a free Redis database [here](https://redis.com/try-free/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f80ac14",
   "metadata": {},
   "source": [
    "The diagram below shows the demo architecture.\n",
    "\n",
    "![](vss-vw-demo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b3e3e0",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bc70abe-fa70-4800-b58e-42174e7641d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: redis in ./venv/lib/python3.11/site-packages (5.0.1)\n",
      "Requirement already satisfied: langchain in ./venv/lib/python3.11/site-packages (0.1.1)\n",
      "Requirement already satisfied: rich in ./venv/lib/python3.11/site-packages (13.7.0)\n",
      "Requirement already satisfied: spacy in ./venv/lib/python3.11/site-packages (3.7.2)\n",
      "Requirement already satisfied: google-cloud-aiplatform in ./venv/lib/python3.11/site-packages (1.39.0)\n",
      "Requirement already satisfied: unstructured in ./venv/lib/python3.11/site-packages (0.12.0)\n",
      "Requirement already satisfied: markdown in ./venv/lib/python3.11/site-packages (3.5.2)\n",
      "Requirement already satisfied: python-dotenv in ./venv/lib/python3.11/site-packages (1.0.0)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./venv/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./venv/lib/python3.11/site-packages (from langchain) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./venv/lib/python3.11/site-packages (from langchain) (3.9.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./venv/lib/python3.11/site-packages (from langchain) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./venv/lib/python3.11/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.13 in ./venv/lib/python3.11/site-packages (from langchain) (0.0.13)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.9 in ./venv/lib/python3.11/site-packages (from langchain) (0.1.12)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.77 in ./venv/lib/python3.11/site-packages (from langchain) (0.0.83)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./venv/lib/python3.11/site-packages (from langchain) (1.26.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./venv/lib/python3.11/site-packages (from langchain) (2.5.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./venv/lib/python3.11/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.11/site-packages (from rich) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.11/site-packages (from rich) (2.17.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./venv/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./venv/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./venv/lib/python3.11/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./venv/lib/python3.11/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./venv/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in ./venv/lib/python3.11/site-packages (from spacy) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./venv/lib/python3.11/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./venv/lib/python3.11/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./venv/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in ./venv/lib/python3.11/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in ./venv/lib/python3.11/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./venv/lib/python3.11/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./venv/lib/python3.11/site-packages (from spacy) (4.66.1)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.11/site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.11/site-packages (from spacy) (69.0.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.11/site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./venv/lib/python3.11/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in ./venv/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.15.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in ./venv/lib/python3.11/site-packages (from google-cloud-aiplatform) (1.23.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in ./venv/lib/python3.11/site-packages (from google-cloud-aiplatform) (4.25.2)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in ./venv/lib/python3.11/site-packages (from google-cloud-aiplatform) (2.14.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in ./venv/lib/python3.11/site-packages (from google-cloud-aiplatform) (3.16.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in ./venv/lib/python3.11/site-packages (from google-cloud-aiplatform) (1.11.0)\n",
      "Requirement already satisfied: shapely<3.0.0dev in ./venv/lib/python3.11/site-packages (from google-cloud-aiplatform) (2.0.2)\n",
      "Requirement already satisfied: chardet in ./venv/lib/python3.11/site-packages (from unstructured) (5.2.0)\n",
      "Requirement already satisfied: filetype in ./venv/lib/python3.11/site-packages (from unstructured) (1.2.0)\n",
      "Requirement already satisfied: python-magic in ./venv/lib/python3.11/site-packages (from unstructured) (0.4.27)\n",
      "Requirement already satisfied: lxml in ./venv/lib/python3.11/site-packages (from unstructured) (5.1.0)\n",
      "Requirement already satisfied: nltk in ./venv/lib/python3.11/site-packages (from unstructured) (3.8.1)\n",
      "Requirement already satisfied: tabulate in ./venv/lib/python3.11/site-packages (from unstructured) (0.9.0)\n",
      "Requirement already satisfied: beautifulsoup4 in ./venv/lib/python3.11/site-packages (from unstructured) (4.12.3)\n",
      "Requirement already satisfied: emoji in ./venv/lib/python3.11/site-packages (from unstructured) (2.10.0)\n",
      "Requirement already satisfied: python-iso639 in ./venv/lib/python3.11/site-packages (from unstructured) (2024.1.2)\n",
      "Requirement already satisfied: langdetect in ./venv/lib/python3.11/site-packages (from unstructured) (1.0.9)\n",
      "Requirement already satisfied: rapidfuzz in ./venv/lib/python3.11/site-packages (from unstructured) (3.6.1)\n",
      "Requirement already satisfied: backoff in ./venv/lib/python3.11/site-packages (from unstructured) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in ./venv/lib/python3.11/site-packages (from unstructured) (4.9.0)\n",
      "Requirement already satisfied: unstructured-client in ./venv/lib/python3.11/site-packages (from unstructured) (0.15.2)\n",
      "Requirement already satisfied: wrapt in ./venv/lib/python3.11/site-packages (from unstructured) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests) (2023.11.17)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in ./venv/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.62.0)\n",
      "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in ./venv/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.26.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in ./venv/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.60.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in ./venv/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.60.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in ./venv/lib/python3.11/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in ./venv/lib/python3.11/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in ./venv/lib/python3.11/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in ./venv/lib/python3.11/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in ./venv/lib/python3.11/site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.5.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in ./venv/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.9->langchain) (4.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in ./venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./venv/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./venv/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./venv/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in ./venv/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./venv/lib/python3.11/site-packages (from beautifulsoup4->unstructured) (2.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: six in ./venv/lib/python3.11/site-packages (from langdetect->unstructured) (1.16.0)\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.11/site-packages (from nltk->unstructured) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./venv/lib/python3.11/site-packages (from nltk->unstructured) (2023.12.25)\n",
      "Requirement already satisfied: jsonpath-python>=1.0.6 in ./venv/lib/python3.11/site-packages (from unstructured-client->unstructured) (1.0.6)\n",
      "Requirement already satisfied: mypy-extensions>=1.0.0 in ./venv/lib/python3.11/site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./venv/lib/python3.11/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain) (1.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./venv/lib/python3.11/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./venv/lib/python3.11/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./venv/lib/python3.11/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in ./venv/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install redis langchain rich spacy google-cloud-aiplatform unstructured markdown python-dotenv requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faae6be4",
   "metadata": {},
   "source": [
    "Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4c7a4eb-b703-4880-8553-d8e57b8b15e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print # this will pretty-print python objects\n",
    "import warnings\n",
    "import dotenv\n",
    "\n",
    "# mute warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# load env vars from .env file\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "def download_file(url, filename):\n",
    "    import requests\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open(filename, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610c9d24",
   "metadata": {},
   "source": [
    "## 0. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0b373c",
   "metadata": {},
   "source": [
    "### Load Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bca13ad",
   "metadata": {},
   "source": [
    "Let's talk to the Redis documentation. We'll load a local copy of the Search [Aggregations](https://redis.io/docs/interact/search-and-query/search/aggregations/) and [Query](https://redis.io/docs/interact/search-and-query/query/) pages and use them to answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7235d10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loaded <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> documents\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loaded \u001b[1;36m2\u001b[0m documents\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load documents\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "# download aggregation doc\n",
    "aggs_url = \"https://github.com/RediSearch/RediSearch/raw/master/docs/docs/advanced-concepts/aggregations.md\"\n",
    "download_file(aggs_url, \"aggregations.md\")\n",
    "aggs_doc_path = \"aggregations.md\"\n",
    "docs = UnstructuredMarkdownLoader(aggs_doc_path).load()\n",
    "\n",
    "# download query syntax doc\n",
    "query_syntax_url = \"https://github.com/RediSearch/RediSearch/raw/master/docs/docs/advanced-concepts/query_syntax.md\"\n",
    "download_file(query_syntax_url, \"query_syntax.md\")\n",
    "query_doc_path = \"query_syntax.md\"\n",
    "docs.extend(UnstructuredMarkdownLoader(query_doc_path).load())\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3c5570",
   "metadata": {},
   "source": [
    "### Split Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e16c4d4",
   "metadata": {},
   "source": [
    "Next, we'll split the doument into chunks and index each chunk as a separate document.\n",
    "\n",
    "This will allow us to retrieve specific, smaller, relevant chunks of the document to add context to our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db2cad70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Generated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">68</span> splits\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Generated \u001b[1;36m68\u001b[0m splits\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split documents into chunks\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "text_splitter = SpacyTextSplitter(chunk_size=750, chunk_overlap=100, strip_whitespace=True)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "print(f\"Generated {len(splits)} splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f242c",
   "metadata": {},
   "source": [
    "### Create Embeddings, Load Into Redis and Create Search Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeb1f0f",
   "metadata": {},
   "source": [
    "Let's create our embeddings transfromer. We will use it to transform our documents, the user's questions, and our prompts into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5830869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model_name will become a required arg for VertexAIEmbeddings starting from Feb-01-2024. Currently the default is set to textembedding-gecko@001\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "\n",
    "# Define Text Embeddings model\n",
    "embedding = VertexAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdb9f7c",
   "metadata": {},
   "source": [
    "We can now use the embeddings object to transform our documents content, then load the documents into Redis.\n",
    "\n",
    "This step will also create a search index called `redis-docs` on the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62489946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 90.9 ms, sys: 9.89 ms, total: 101 ms\n",
      "Wall time: 1.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create embeddings and load data into Redis\n",
    "from langchain.vectorstores import Redis\n",
    "\n",
    "vectordb = Redis.from_documents(documents=splits, embedding=embedding, index_name=\"redis-docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7792474f",
   "metadata": {},
   "source": [
    "### Test: Retrieve Documents Related to a Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6dd861cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How can I load the redis key name (Document ID) and filter results based on that field?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92436ebc",
   "metadata": {},
   "source": [
    "*K* is the number of documents to retrieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eab9031e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'LOAD hurts the performance of aggregate queries considerably since every processed record</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">needs to execute the equivalent of HMGET against a Redis key, which when executed over millions of keys, amounts to</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">very high processing times.\\n\\n\\nThe document ID can be loaded using @__key.\\n\\n\\n\\nGROUPBY {nargs} {property} ... </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:\\n\\nGroup the results in the pipeline based on one or more properties.\\n\\nEach group should have at least one </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reducer (See below), a function that handles the group entries, either counting them or performing multiple </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">aggregate operations (see below).\\n\\n\\n\\nREDUCE {func} {nargs} {arg} ...\\n\\n[AS {name}]: Reduce the matching </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">results in each group into a single record, using a reduction function.'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'doc:redis-docs:56310785040443b79164be2c635a1fbc'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'aggregations.md'</span><span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2201</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'@url|image:mydomain\"\\n\\nNow, you search for documents that have \"hello\" and \"world\" </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">either in the body or the title and the term mydomain in their url or image fields.\\n\\n\\n\\nNumeric filters in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">query\\n\\nIf a field in the schema is defined as NUMERIC, it is possible to use the FILTER argument in the Redis </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">request or filter with it by specifying filtering rules in the query.\\n\\nThe syntax is @field:[{min} {max}], for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">example, @price:[100 200].\\n\\n\\n\\nA few notes on numeric predicates\\n\\nIt is possible to specify a numeric </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">predicate as the entire query, whereas it is impossible to do it with the FILTER argument.\\n\\n\\n\\nIt is possible to</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">intersect or union multiple numeric filters in the same query, be it for the same field or different ones.'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'doc:redis-docs:8ab9749cac1c4ceeaa43fddf96901ec8'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'query_syntax.md'</span><span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2468</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The first element is\\nthe actual (partial) result, and the second is the cursor </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ID.\\n\\nThe cursor ID\\ncan then be fed to FT.CURSOR READ repeatedly until the cursor ID is 0, in\\nwhich case all </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">results have been returned.\\n\\n\\n\\nTo read from an existing cursor, use FT.CURSOR READ.\\n\\nFor </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">example:\\n\\nFT.CURSOR READ idx 342459320\\n\\nAssuming 342459320 is the cursor ID returned from the FT.AGGREGATE </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">request, here is an example in pseudo-code:\\n\\nresponse, cursor = FT.AGGREGATE \"idx\" \"redis\" \"WITHCURSOR\";\\nwhile </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(1) {\\n  processResponse(response)\\n  if (!cursor) {\\n    break;\\n  }\\n  response, cursor = FT.CURSOR read \"idx\" </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">cursor\\n}\\n\\nNote that even if the cursor is 0, a partial result may still be returned.'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'doc:redis-docs:fd2547b209a44dfab0486320d08917d2'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'aggregations.md'</span><span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2663</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'LOAD hurts the performance of aggregate queries considerably since every processed record\u001b[0m\n",
       "\u001b[32mneeds to execute the equivalent of HMGET against a Redis key, which when executed over millions of keys, amounts to\u001b[0m\n",
       "\u001b[32mvery high processing times.\\n\\n\\nThe document ID can be loaded using @__key.\\n\\n\\n\\nGROUPBY \u001b[0m\u001b[32m{\u001b[0m\u001b[32mnargs\u001b[0m\u001b[32m}\u001b[0m\u001b[32m \u001b[0m\u001b[32m{\u001b[0m\u001b[32mproperty\u001b[0m\u001b[32m}\u001b[0m\u001b[32m ... \u001b[0m\n",
       "\u001b[32m:\\n\\nGroup the results in the pipeline based on one or more properties.\\n\\nEach group should have at least one \u001b[0m\n",
       "\u001b[32mreducer \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSee below\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, a function that handles the group entries, either counting them or performing multiple \u001b[0m\n",
       "\u001b[32maggregate operations \u001b[0m\u001b[32m(\u001b[0m\u001b[32msee below\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n\\n\\nREDUCE \u001b[0m\u001b[32m{\u001b[0m\u001b[32mfunc\u001b[0m\u001b[32m}\u001b[0m\u001b[32m \u001b[0m\u001b[32m{\u001b[0m\u001b[32mnargs\u001b[0m\u001b[32m}\u001b[0m\u001b[32m \u001b[0m\u001b[32m{\u001b[0m\u001b[32marg\u001b[0m\u001b[32m}\u001b[0m\u001b[32m ...\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAS \u001b[0m\u001b[32m{\u001b[0m\u001b[32mname\u001b[0m\u001b[32m}\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: Reduce the matching \u001b[0m\n",
       "\u001b[32mresults in each group into a single record, using a reduction function.'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \u001b[32m'doc:redis-docs:56310785040443b79164be2c635a1fbc'\u001b[0m, \u001b[32m'source'\u001b[0m: \u001b[32m'aggregations.md'\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;36m0.2201\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1m(\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'@url|image:mydomain\"\\n\\nNow, you search for documents that have \"hello\" and \"world\" \u001b[0m\n",
       "\u001b[32meither in the body or the title and the term mydomain in their url or image fields.\\n\\n\\n\\nNumeric filters in \u001b[0m\n",
       "\u001b[32mquery\\n\\nIf a field in the schema is defined as NUMERIC, it is possible to use the FILTER argument in the Redis \u001b[0m\n",
       "\u001b[32mrequest or filter with it by specifying filtering rules in the query.\\n\\nThe syntax is @field:\u001b[0m\u001b[32m[\u001b[0m\u001b[32m{\u001b[0m\u001b[32mmin\u001b[0m\u001b[32m}\u001b[0m\u001b[32m \u001b[0m\u001b[32m{\u001b[0m\u001b[32mmax\u001b[0m\u001b[32m}\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, for \u001b[0m\n",
       "\u001b[32mexample, @price:\u001b[0m\u001b[32m[\u001b[0m\u001b[32m100 200\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\n\\n\\n\\nA few notes on numeric predicates\\n\\nIt is possible to specify a numeric \u001b[0m\n",
       "\u001b[32mpredicate as the entire query, whereas it is impossible to do it with the FILTER argument.\\n\\n\\n\\nIt is possible to\u001b[0m\n",
       "\u001b[32mintersect or union multiple numeric filters in the same query, be it for the same field or different ones.'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \u001b[32m'doc:redis-docs:8ab9749cac1c4ceeaa43fddf96901ec8'\u001b[0m, \u001b[32m'source'\u001b[0m: \u001b[32m'query_syntax.md'\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;36m0.2468\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1m(\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'The first element is\\nthe actual \u001b[0m\u001b[32m(\u001b[0m\u001b[32mpartial\u001b[0m\u001b[32m)\u001b[0m\u001b[32m result, and the second is the cursor \u001b[0m\n",
       "\u001b[32mID.\\n\\nThe cursor ID\\ncan then be fed to FT.CURSOR READ repeatedly until the cursor ID is 0, in\\nwhich case all \u001b[0m\n",
       "\u001b[32mresults have been returned.\\n\\n\\n\\nTo read from an existing cursor, use FT.CURSOR READ.\\n\\nFor \u001b[0m\n",
       "\u001b[32mexample:\\n\\nFT.CURSOR READ idx 342459320\\n\\nAssuming 342459320 is the cursor ID returned from the FT.AGGREGATE \u001b[0m\n",
       "\u001b[32mrequest, here is an example in pseudo-code:\\n\\nresponse, cursor = FT.AGGREGATE \"idx\" \"redis\" \"WITHCURSOR\";\\nwhile \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32m1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n  processResponse\u001b[0m\u001b[32m(\u001b[0m\u001b[32mresponse\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n  if \u001b[0m\u001b[32m(\u001b[0m\u001b[32m!cursor\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n    break;\\n  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n  response, cursor = FT.CURSOR read \"idx\" \u001b[0m\n",
       "\u001b[32mcursor\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\\nNote that even if the cursor is 0, a partial result may still be returned.'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \u001b[32m'doc:redis-docs:fd2547b209a44dfab0486320d08917d2'\u001b[0m, \u001b[32m'source'\u001b[0m: \u001b[32m'aggregations.md'\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;36m0.2663\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = vectordb.similarity_search_with_score(question, k=3)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe89eac",
   "metadata": {},
   "source": [
    "A different type of search is Max Marginal Relevance (MMR) search. MMR search is an algorithm that combines the similarity of a document to a query with the similarity of the document to the other documents in the result set. It is useful when you want to retrieve a set of documents that are similar to a query, but also diverse from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1bcb261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'LOAD hurts the performance of aggregate queries considerably since every processed record </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">needs to execute the equivalent of HMGET against a Redis key, which when executed over millions of keys, amounts to</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">very high processing times.\\n\\n\\nThe document ID can be loaded using @__key.\\n\\n\\n\\nGROUPBY {nargs} {property} ... </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:\\n\\nGroup the results in the pipeline based on one or more properties.\\n\\nEach group should have at least one </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reducer (See below), a function that handles the group entries, either counting them or performing multiple </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">aggregate operations (see below).\\n\\n\\n\\nREDUCE {func} {nargs} {arg} ...\\n\\n[AS {name}]: Reduce the matching </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">results in each group into a single record, using a reduction function.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'doc:redis-docs:56310785040443b79164be2c635a1fbc'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'aggregations.md'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'@url|image:mydomain\"\\n\\nNow, you search for documents that have \"hello\" and \"world\" either in</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the body or the title and the term mydomain in their url or image fields.\\n\\n\\n\\nNumeric filters in query\\n\\nIf a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">field in the schema is defined as NUMERIC, it is possible to use the FILTER argument in the Redis request or filter</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with it by specifying filtering rules in the query.\\n\\nThe syntax is @field:[{min} {max}], for example, @price:[100</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">200].\\n\\n\\n\\nA few notes on numeric predicates\\n\\nIt is possible to specify a numeric predicate as the entire </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">query, whereas it is impossible to do it with the FILTER argument.\\n\\n\\n\\nIt is possible to intersect or union </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">multiple numeric filters in the same query, be it for the same field or different ones.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'doc:redis-docs:8ab9749cac1c4ceeaa43fddf96901ec8'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'query_syntax.md'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The first element is\\nthe actual (partial) result, and the second is the cursor ID.\\n\\nThe </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">cursor ID\\ncan then be fed to FT.CURSOR READ repeatedly until the cursor ID is 0, in\\nwhich case all results have </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">been returned.\\n\\n\\n\\nTo read from an existing cursor, use FT.CURSOR READ.\\n\\nFor example:\\n\\nFT.CURSOR READ idx </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">342459320\\n\\nAssuming 342459320 is the cursor ID returned from the FT.AGGREGATE request, here is an example in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pseudo-code:\\n\\nresponse, cursor = FT.AGGREGATE \"idx\" \"redis\" \"WITHCURSOR\";\\nwhile (1) {\\n  </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">processResponse(response)\\n  if (!cursor) {\\n    break;\\n  }\\n  response, cursor = FT.CURSOR read \"idx\" </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">cursor\\n}\\n\\nNote that even if the cursor is 0, a partial result may still be returned.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'doc:redis-docs:fd2547b209a44dfab0486320d08917d2'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'aggregations.md'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'LOAD hurts the performance of aggregate queries considerably since every processed record \u001b[0m\n",
       "\u001b[32mneeds to execute the equivalent of HMGET against a Redis key, which when executed over millions of keys, amounts to\u001b[0m\n",
       "\u001b[32mvery high processing times.\\n\\n\\nThe document ID can be loaded using @__key.\\n\\n\\n\\nGROUPBY \u001b[0m\u001b[32m{\u001b[0m\u001b[32mnargs\u001b[0m\u001b[32m}\u001b[0m\u001b[32m \u001b[0m\u001b[32m{\u001b[0m\u001b[32mproperty\u001b[0m\u001b[32m}\u001b[0m\u001b[32m ... \u001b[0m\n",
       "\u001b[32m:\\n\\nGroup the results in the pipeline based on one or more properties.\\n\\nEach group should have at least one \u001b[0m\n",
       "\u001b[32mreducer \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSee below\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, a function that handles the group entries, either counting them or performing multiple \u001b[0m\n",
       "\u001b[32maggregate operations \u001b[0m\u001b[32m(\u001b[0m\u001b[32msee below\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n\\n\\nREDUCE \u001b[0m\u001b[32m{\u001b[0m\u001b[32mfunc\u001b[0m\u001b[32m}\u001b[0m\u001b[32m \u001b[0m\u001b[32m{\u001b[0m\u001b[32mnargs\u001b[0m\u001b[32m}\u001b[0m\u001b[32m \u001b[0m\u001b[32m{\u001b[0m\u001b[32marg\u001b[0m\u001b[32m}\u001b[0m\u001b[32m ...\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAS \u001b[0m\u001b[32m{\u001b[0m\u001b[32mname\u001b[0m\u001b[32m}\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: Reduce the matching \u001b[0m\n",
       "\u001b[32mresults in each group into a single record, using a reduction function.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \u001b[32m'doc:redis-docs:56310785040443b79164be2c635a1fbc'\u001b[0m, \u001b[32m'source'\u001b[0m: \u001b[32m'aggregations.md'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'@url|image:mydomain\"\\n\\nNow, you search for documents that have \"hello\" and \"world\" either in\u001b[0m\n",
       "\u001b[32mthe body or the title and the term mydomain in their url or image fields.\\n\\n\\n\\nNumeric filters in query\\n\\nIf a \u001b[0m\n",
       "\u001b[32mfield in the schema is defined as NUMERIC, it is possible to use the FILTER argument in the Redis request or filter\u001b[0m\n",
       "\u001b[32mwith it by specifying filtering rules in the query.\\n\\nThe syntax is @field:\u001b[0m\u001b[32m[\u001b[0m\u001b[32m{\u001b[0m\u001b[32mmin\u001b[0m\u001b[32m}\u001b[0m\u001b[32m \u001b[0m\u001b[32m{\u001b[0m\u001b[32mmax\u001b[0m\u001b[32m}\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, for example, @price:\u001b[0m\u001b[32m[\u001b[0m\u001b[32m100\u001b[0m\n",
       "\u001b[32m200\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\n\\n\\n\\nA few notes on numeric predicates\\n\\nIt is possible to specify a numeric predicate as the entire \u001b[0m\n",
       "\u001b[32mquery, whereas it is impossible to do it with the FILTER argument.\\n\\n\\n\\nIt is possible to intersect or union \u001b[0m\n",
       "\u001b[32mmultiple numeric filters in the same query, be it for the same field or different ones.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \u001b[32m'doc:redis-docs:8ab9749cac1c4ceeaa43fddf96901ec8'\u001b[0m, \u001b[32m'source'\u001b[0m: \u001b[32m'query_syntax.md'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'The first element is\\nthe actual \u001b[0m\u001b[32m(\u001b[0m\u001b[32mpartial\u001b[0m\u001b[32m)\u001b[0m\u001b[32m result, and the second is the cursor ID.\\n\\nThe \u001b[0m\n",
       "\u001b[32mcursor ID\\ncan then be fed to FT.CURSOR READ repeatedly until the cursor ID is 0, in\\nwhich case all results have \u001b[0m\n",
       "\u001b[32mbeen returned.\\n\\n\\n\\nTo read from an existing cursor, use FT.CURSOR READ.\\n\\nFor example:\\n\\nFT.CURSOR READ idx \u001b[0m\n",
       "\u001b[32m342459320\\n\\nAssuming 342459320 is the cursor ID returned from the FT.AGGREGATE request, here is an example in \u001b[0m\n",
       "\u001b[32mpseudo-code:\\n\\nresponse, cursor = FT.AGGREGATE \"idx\" \"redis\" \"WITHCURSOR\";\\nwhile \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n  \u001b[0m\n",
       "\u001b[32mprocessResponse\u001b[0m\u001b[32m(\u001b[0m\u001b[32mresponse\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n  if \u001b[0m\u001b[32m(\u001b[0m\u001b[32m!cursor\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n    break;\\n  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n  response, cursor = FT.CURSOR read \"idx\" \u001b[0m\n",
       "\u001b[32mcursor\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\\nNote that even if the cursor is 0, a partial result may still be returned.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \u001b[32m'doc:redis-docs:fd2547b209a44dfab0486320d08917d2'\u001b[0m, \u001b[32m'source'\u001b[0m: \u001b[32m'aggregations.md'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = vectordb.max_marginal_relevance_search(question, k=3, top_k=5, threshold=0.5)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f307cf",
   "metadata": {},
   "source": [
    "## 1. Semantic Search - Question Answering (Q&A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54870447",
   "metadata": {},
   "source": [
    "We will create a prommpt template that will provide instructions to the LLM,\n",
    "as well as contain placeholders for the context (retrieved from Redis) and the question (asked by the user)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80dccb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "QA_TEMPLATE = \"\"\"\n",
    "Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum. Keep the answer as concise as possible. \n",
    "-----\n",
    "Context: \n",
    "\n",
    "{context}\n",
    "-----\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], \n",
    "    template=QA_TEMPLATE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b4b3a4",
   "metadata": {},
   "source": [
    "Next we will create an LLM object and use it to generate answers to our questions.\n",
    "\n",
    "We will also create the `RetrievalQA` chain, which will retrieve the most relevant documents from Redis, and use them as context for the LLM.\n",
    "\n",
    "We are specifying:\n",
    "* The LLM model name (`text-bison`)\n",
    "* The maximum length of the generated answer (`max_output_tokens`)\n",
    "* The LLM temperature (`temperature`), which controls the randomness of the generated text. Higher values will result in more random text while lower values will result in more predictable text.\n",
    "\n",
    "The type of chain we're creating is a `stuff` chain, as in \"stuff the retrieved documents into the LLM\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "beb248a1-f56d-4dde-9f8a-ad32a5089f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import VertexAI\n",
    "\n",
    "# Define LLM to generate response\n",
    "llm = VertexAI(model_name='text-bison@001', max_output_tokens=512, temperature=0.5)\n",
    "\n",
    "# Create QA chain to respond to user query along with source documents\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7bf0aa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How can I load the redis key name (Document ID) and filter results based on that field?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67414866-4785-4488-b5e5-fb9cbef5e691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score_threshold is deprecated. Use distance_threshold instead.score_threshold should only be used in similarity_search_with_relevance_scores.score_threshold will be removed in a future release.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">We can load the redis key name using @__key.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "We can load the redis key name using @__key.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.4 ms, sys: 8 ms, total: 36.4 ms\n",
      "Wall time: 970 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run QA chain\n",
    "result = qa({\"query\": question})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aff2925",
   "metadata": {},
   "source": [
    "## 2. Semantic Cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1094323",
   "metadata": {},
   "source": [
    "Making calls to a (paid) LLM API can get very expensive, very quickly. We can use Redis to cache the results of our LLM calls, and use the cache to answer questions that we've already answered before.\n",
    "\n",
    "This will not only save on API usage costs, but will also significantly speed up our response times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e471f9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.cache import RedisSemanticCache\n",
    "\n",
    "langchain.llm_cache = RedisSemanticCache(\n",
    "    embedding=embedding,\n",
    "    redis_url=\"redis://localhost:6379\",\n",
    "    score_threshold=0.2  # what is the maximum distance between the query and the retrieved document\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40009aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How do I get documents withing a certain radius from a point?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e33c66cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score_threshold is deprecated. Use distance_threshold instead.score_threshold should only be used in similarity_search_with_relevance_scores.score_threshold will be removed in a future release.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">To get documents withing a certain radius from a point, you can use the geo radius query with the syntax \n",
       "@field:<span style=\"font-weight: bold\">[{</span>lon<span style=\"font-weight: bold\">}</span> <span style=\"font-weight: bold\">{</span>lat<span style=\"font-weight: bold\">}</span> <span style=\"font-weight: bold\">{</span>radius<span style=\"font-weight: bold\">}</span> <span style=\"font-weight: bold\">{</span>m|km|mi|ft<span style=\"font-weight: bold\">}]</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "To get documents withing a certain radius from a point, you can use the geo radius query with the syntax \n",
       "@field:\u001b[1m[\u001b[0m\u001b[1m{\u001b[0mlon\u001b[1m}\u001b[0m \u001b[1m{\u001b[0mlat\u001b[1m}\u001b[0m \u001b[1m{\u001b[0mradius\u001b[1m}\u001b[0m \u001b[1m{\u001b[0mm|km|mi|ft\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48 ms, sys: 9.74 ms, total: 57.7 ms\n",
      "Wall time: 1.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = qa({\"query\": question})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e36d5031",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How do I get documents withing a certain radius from a coordinate?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9f6f8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score_threshold is deprecated. Use distance_threshold instead.score_threshold should only be used in similarity_search_with_relevance_scores.score_threshold will be removed in a future release.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">To get documents withing a certain radius from a point, you can use the geo radius query with the syntax \n",
       "@field:<span style=\"font-weight: bold\">[{</span>lon<span style=\"font-weight: bold\">}</span> <span style=\"font-weight: bold\">{</span>lat<span style=\"font-weight: bold\">}</span> <span style=\"font-weight: bold\">{</span>radius<span style=\"font-weight: bold\">}</span> <span style=\"font-weight: bold\">{</span>m|km|mi|ft<span style=\"font-weight: bold\">}]</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "To get documents withing a certain radius from a point, you can use the geo radius query with the syntax \n",
       "@field:\u001b[1m[\u001b[0m\u001b[1m{\u001b[0mlon\u001b[1m}\u001b[0m \u001b[1m{\u001b[0mlat\u001b[1m}\u001b[0m \u001b[1m{\u001b[0mradius\u001b[1m}\u001b[0m \u001b[1m{\u001b[0mm|km|mi|ft\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.5 ms, sys: 7.15 ms, total: 27.6 ms\n",
      "Wall time: 319 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "result = qa({\"query\": question})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a736d28b",
   "metadata": {},
   "source": [
    "## 3. Chat Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028c3563",
   "metadata": {},
   "source": [
    "In this use case, we'll use Redis to provide a memory to our chatbot. We'll use the memory to store the user's questions and the LLM's answers, and use them to provide context to the LLM in subsequent questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3c95687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "\n",
    "# Clear cache\n",
    "langchain.llm_cache = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7323976c",
   "metadata": {},
   "source": [
    "## I Do Not Recall\n",
    "\n",
    "First, let's have a chat with the LLM ***without*** any memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9832e65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import VertexAI\n",
    "\n",
    "# Define LLM to generate response\n",
    "llm = VertexAI(model_name='text-bison@001', max_output_tokens=512, temperature=0.2)\n",
    "\n",
    "template = \"\"\"Assistant is a large language model, designed to be able to assist with a wide range of tasks, \n",
    "from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. \n",
    "As a language model, Assistant is able to generate human-like text based on the input it receives, \n",
    "allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
    "\n",
    "Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a \n",
    "wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, \n",
    "Assistant is here to assist.\n",
    "\n",
    "Human: {human_input}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"human_input\"], \n",
    "    template=template\n",
    "    )\n",
    "\n",
    "# Create QA chain to respond to user query along with source documents\n",
    "chat = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd20009",
   "metadata": {},
   "source": [
    "Using `verbose=True`, we can see the LLM's context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e39ff809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAssistant is a large language model, designed to be able to assist with a wide range of tasks, \n",
      "from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. \n",
      "As a language model, Assistant is able to generate human-like text based on the input it receives, \n",
      "allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a \n",
      "wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, \n",
      "Assistant is here to assist.\n",
      "\n",
      "Human: Hi, my name is Eli. I like eating noodles and I work at Redis. What is your name?\n",
      "Assistant:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">It is nice to meet you Eli. My name is Assistant, and I am powered by PaLM <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, which stands for Pathways Language \n",
       "Model <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. PaLM <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> was trained by a team of engineers and scientists at Google AI, and it is designed to understand \n",
       "and generate human language.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "It is nice to meet you Eli. My name is Assistant, and I am powered by PaLM \u001b[1;36m2\u001b[0m, which stands for Pathways Language \n",
       "Model \u001b[1;36m2\u001b[0m. PaLM \u001b[1;36m2\u001b[0m was trained by a team of engineers and scientists at Google AI, and it is designed to understand \n",
       "and generate human language.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reply = chat.predict(human_input=\"Hi, my name is Eli. I like eating noodles and I work at Redis. What is your name?\")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54922686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAssistant is a large language model, designed to be able to assist with a wide range of tasks, \n",
      "from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. \n",
      "As a language model, Assistant is able to generate human-like text based on the input it receives, \n",
      "allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a \n",
      "wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, \n",
      "Assistant is here to assist.\n",
      "\n",
      "Human: Who won the World Cup in 2018?\n",
      "Assistant:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2018</span> FIFA World Cup was the 21st FIFA World Cup, an international football tournament contested by the men's \n",
       "national teams of the member associations of FIFA. The tournament took place in Russia from <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span> June to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span> July \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2018</span>. France won the tournament for the second time, defeating Croatia <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> in the final.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The \u001b[1;36m2018\u001b[0m FIFA World Cup was the 21st FIFA World Cup, an international football tournament contested by the men's \n",
       "national teams of the member associations of FIFA. The tournament took place in Russia from \u001b[1;36m14\u001b[0m June to \u001b[1;36m15\u001b[0m July \n",
       "\u001b[1;36m2018\u001b[0m. France won the tournament for the second time, defeating Croatia \u001b[1;36m4\u001b[0m\u001b[1;36m2\u001b[0m in the final.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reply = chat.predict(human_input=\"Who won the World Cup in 2018?\")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec658166",
   "metadata": {},
   "source": [
    "If we had memory, the LLM would know the answer to the next question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2310d2a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAssistant is a large language model, designed to be able to assist with a wide range of tasks, \n",
      "from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. \n",
      "As a language model, Assistant is able to generate human-like text based on the input it receives, \n",
      "allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a \n",
      "wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, \n",
      "Assistant is here to assist.\n",
      "\n",
      "Human: What's my name?\n",
      "Assistant:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">I am not sure what your name is. Can you tell me?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "I am not sure what your name is. Can you tell me?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reply = chat.predict(human_input=\"What's my name?\")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1e61d9",
   "metadata": {},
   "source": [
    "---\n",
    "## Total Recall\n",
    "Now let's build the same chatbot ***with*** memory.\n",
    "\n",
    "The message history will be stored in Redis, and the LLM will use it to provide context to the next question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "807c7c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import RedisChatMessageHistory, ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Define LLM to generate response\n",
    "llm = VertexAI(model_name='text-bison', max_output_tokens=512, temperature=0.4)\n",
    "\n",
    "template = \"\"\"Assistant is a large language model, designed to be able to assist with a wide range of tasks, \n",
    "from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. \n",
    "As a language model, Assistant is able to generate human-like text based on the input it receives, \n",
    "allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
    "\n",
    "Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a \n",
    "wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, \n",
    "Assistant is here to assist.\n",
    "----------\n",
    "{history}\n",
    "----------\n",
    "Human: {human_input}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"human_input\"], \n",
    "    template=template\n",
    "    )\n",
    "\n",
    "# define the chat message memory\n",
    "message_history = RedisChatMessageHistory(key_prefix=\"chat-history:\", session_id=\"vs-demo\")\n",
    "message_history.clear()\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"history\", chat_memory=message_history\n",
    ")\n",
    "\n",
    "# Create QA chain to respond to user query along with source documents\n",
    "chat = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0706d1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAssistant is a large language model, designed to be able to assist with a wide range of tasks, \n",
      "from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. \n",
      "As a language model, Assistant is able to generate human-like text based on the input it receives, \n",
      "allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a \n",
      "wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, \n",
      "Assistant is here to assist.\n",
      "----------\n",
      "\n",
      "----------\n",
      "Human: Hi, my name is Adam. I have 3 kids and I like gardening. What is your name?\n",
      "Assistant:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Hello Adam! My name is Assistant, and I'm a large language model designed to help with a wide range of tasks and \n",
       "provide information on a variety of topics. I don't have a physical body or personal experiences like humans do, \n",
       "but I'm here to assist you with any questions or tasks you may have. Is there anything I can help you with today?\n",
       "</pre>\n"
      ],
      "text/plain": [
       " Hello Adam! My name is Assistant, and I'm a large language model designed to help with a wide range of tasks and \n",
       "provide information on a variety of topics. I don't have a physical body or personal experiences like humans do, \n",
       "but I'm here to assist you with any questions or tasks you may have. Is there anything I can help you with today?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reply = chat.predict(human_input=\"Hi, my name is Adam. I have 3 kids and I like gardening. What is your name?\")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd12418b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAssistant is a large language model, designed to be able to assist with a wide range of tasks, \n",
      "from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. \n",
      "As a language model, Assistant is able to generate human-like text based on the input it receives, \n",
      "allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a \n",
      "wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, \n",
      "Assistant is here to assist.\n",
      "----------\n",
      "Human: Hi, my name is Adam. I have 3 kids and I like gardening. What is your name?\n",
      "AI:  Hello Adam! My name is Assistant, and I'm a large language model designed to help with a wide range of tasks and provide information on a variety of topics. I don't have a physical body or personal experiences like humans do, but I'm here to assist you with any questions or tasks you may have. Is there anything I can help you with today?\n",
      "----------\n",
      "Human: How long was the last Harry Potter book?\n",
      "Assistant:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> The last Harry Potter book, <span style=\"color: #008000; text-decoration-color: #008000\">\"Harry Potter and the Deathly Hallows,\"</span> was <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">759</span> pages long in the UK edition and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">784</span> \n",
       "pages long in the US edition. It was released on July <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2007</span>, and is the seventh and final novel in the Harry \n",
       "Potter series written by J.K. Rowling.\n",
       "</pre>\n"
      ],
      "text/plain": [
       " The last Harry Potter book, \u001b[32m\"Harry Potter and the Deathly Hallows,\"\u001b[0m was \u001b[1;36m759\u001b[0m pages long in the UK edition and \u001b[1;36m784\u001b[0m \n",
       "pages long in the US edition. It was released on July \u001b[1;36m21\u001b[0m, \u001b[1;36m2007\u001b[0m, and is the seventh and final novel in the Harry \n",
       "Potter series written by J.K. Rowling.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reply = chat.predict(human_input=\"How long was the last Harry Potter book?\")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a50c708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAssistant is a large language model, designed to be able to assist with a wide range of tasks, \n",
      "from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. \n",
      "As a language model, Assistant is able to generate human-like text based on the input it receives, \n",
      "allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a \n",
      "wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, \n",
      "Assistant is here to assist.\n",
      "----------\n",
      "Human: Hi, my name is Adam. I have 3 kids and I like gardening. What is your name?\n",
      "AI:  Hello Adam! My name is Assistant, and I'm a large language model designed to help with a wide range of tasks and provide information on a variety of topics. I don't have a physical body or personal experiences like humans do, but I'm here to assist you with any questions or tasks you may have. Is there anything I can help you with today?\n",
      "Human: How long was the last Harry Potter book?\n",
      "AI:  The last Harry Potter book, \"Harry Potter and the Deathly Hallows,\" was 759 pages long in the UK edition and 784 pages long in the US edition. It was released on July 21, 2007, and is the seventh and final novel in the Harry Potter series written by J.K. Rowling.\n",
      "----------\n",
      "Human: What's the name of their school?\n",
      "Assistant:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> The school's name is Hogwarts School of Witchcraft and Wizardry.\n",
       "</pre>\n"
      ],
      "text/plain": [
       " The school's name is Hogwarts School of Witchcraft and Wizardry.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reply = chat.predict(human_input=\"What's the name of their school?\")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c6c6e6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAssistant is a large language model, designed to be able to assist with a wide range of tasks, \n",
      "from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. \n",
      "As a language model, Assistant is able to generate human-like text based on the input it receives, \n",
      "allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a \n",
      "wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, \n",
      "Assistant is here to assist.\n",
      "----------\n",
      "Human: Hi, my name is Adam. I have 3 kids and I like gardening. What is your name?\n",
      "AI:  Hello Adam! My name is Assistant, and I'm a large language model designed to help with a wide range of tasks and provide information on a variety of topics. I don't have a physical body or personal experiences like humans do, but I'm here to assist you with any questions or tasks you may have. Is there anything I can help you with today?\n",
      "Human: How long was the last Harry Potter book?\n",
      "AI:  The last Harry Potter book, \"Harry Potter and the Deathly Hallows,\" was 759 pages long in the UK edition and 784 pages long in the US edition. It was released on July 21, 2007, and is the seventh and final novel in the Harry Potter series written by J.K. Rowling.\n",
      "Human: What's the name of their school?\n",
      "AI:  The school's name is Hogwarts School of Witchcraft and Wizardry.\n",
      "----------\n",
      "Human: What train platform was the train on?\n",
      "Assistant:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> The train was on platform 9.\n",
       "</pre>\n"
      ],
      "text/plain": [
       " The train was on platform 9.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reply = chat.predict(human_input=\"What train platform was the train on?\")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2dfb571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAssistant is a large language model, designed to be able to assist with a wide range of tasks, \n",
      "from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. \n",
      "As a language model, Assistant is able to generate human-like text based on the input it receives, \n",
      "allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a \n",
      "wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, \n",
      "Assistant is here to assist.\n",
      "----------\n",
      "Human: Hi, my name is Adam. I have 3 kids and I like gardening. What is your name?\n",
      "AI:  Hello Adam! My name is Assistant, and I'm a large language model designed to help with a wide range of tasks and provide information on a variety of topics. I don't have a physical body or personal experiences like humans do, but I'm here to assist you with any questions or tasks you may have. Is there anything I can help you with today?\n",
      "Human: How long was the last Harry Potter book?\n",
      "AI:  The last Harry Potter book, \"Harry Potter and the Deathly Hallows,\" was 759 pages long in the UK edition and 784 pages long in the US edition. It was released on July 21, 2007, and is the seventh and final novel in the Harry Potter series written by J.K. Rowling.\n",
      "Human: What's the name of their school?\n",
      "AI:  The school's name is Hogwarts School of Witchcraft and Wizardry.\n",
      "Human: What train platform was the train on?\n",
      "AI:  The train was on platform 9.\n",
      "----------\n",
      "Human: Do you remember my name?\n",
      "Assistant:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Yes, I remember your name. You are Adam. Is there anything else I can help you with?\n",
       "</pre>\n"
      ],
      "text/plain": [
       " Yes, I remember your name. You are Adam. Is there anything else I can help you with?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reply = chat.predict(human_input=\"Do you remember my name?\")\n",
    "print(reply)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
